INFO dispatcher-event-loop-10 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on 192.168.145.1:61945 in memory (size: 1866.0 B, free: 4.1 GB)
INFO JobGenerator org.apache.spark.rdd.ShuffledRDD - Removing RDD 7 from persistence list
INFO block-manager-slave-async-thread-pool-9 org.apache.spark.storage.BlockManager - Removing RDD 7
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 6 from persistence list
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 6
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 5 from persistence list
INFO dispatcher-event-loop-9 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on 192.168.145.1:61945 in memory (size: 1865.0 B, free: 4.1 GB)
INFO block-manager-slave-async-thread-pool-7 org.apache.spark.storage.BlockManager - Removing RDD 5
INFO JobGenerator org.apache.spark.streaming.kafka010.KafkaRDD - Removing RDD 4 from persistence list
INFO block-manager-slave-async-thread-pool-5 org.apache.spark.storage.BlockManager - Removing RDD 4
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877035000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877035000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877035000 ms
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877035000 ms to writer queue
INFO pool-17-thread-6 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877035000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877035000'
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877040000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877040000 ms.0 from job set of time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877040000 ms to writer queue
INFO pool-17-thread-7 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877040000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877040000'
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 3 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-7 org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.145.1:61945 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 6 tasks
INFO dispatcher-event-loop-9 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 24, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-9 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 25, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 25)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 24)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 0 offsets 15075 -> 15095
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 5 offsets 15236 -> 15253
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initializing cache 16 64 0.75
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,0)
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

WARN Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,5)
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-3
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

WARN Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 0 15075
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 5 15236
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=V]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=XXXX]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892499617933046]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0711244300]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=R]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=1]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=M]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=9999]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892495120622861]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=1109121500]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 6.0 (TID 24)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 6.0 (TID 25)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 26, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 26)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 27, localhost, executor driver, partition 3, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 27)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 1 offsets 25860 -> 25880
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,1)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 4 offsets 31250 -> 31260
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-4
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

WARN Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,4)
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 1 25860
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-5
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
WARN Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 4 31250
WARN task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 6.0 (TID 25, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=B]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892493659321581]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0705041000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 2.0 in stage 6.0 (TID 26)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=I]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=1050]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892493626624894]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0624095600]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=3]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 3.0 in stage 6.0 (TID 27)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 6.0 failed 1 times; aborting job
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 6.0 (TID 24) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 1]
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Lost task 2.0 in stage 6.0 (TID 26) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 2]
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Lost task 3.0 in stage 6.0 (TID 27) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 3]
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 6
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) failed in 0.253 s due to Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 25, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 3 failed: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.261669 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877040000 ms.0 from job set of time 1542877040000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.304 s for time 1542877040000 ms (execution: 0.266 s)
INFO JobGenerator org.apache.spark.rdd.ShuffledRDD - Removing RDD 11 from persistence list
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 11
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 10 from persistence list
INFO block-manager-slave-async-thread-pool-5 org.apache.spark.storage.BlockManager - Removing RDD 10
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 9 from persistence list
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 9
INFO JobGenerator org.apache.spark.streaming.kafka010.KafkaRDD - Removing RDD 8 from persistence list
ERROR JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Error running job streaming job 1542877040000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 6.0 failed 1 times, most recent failure: Lost task 1.0 in stage 6.0 (TID 25, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)
	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:144)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	... 3 more
INFO block-manager-slave-async-thread-pool-5 org.apache.spark.storage.BlockManager - Removing RDD 8
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877040000 ms
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877040000 ms to writer queue
INFO pool-17-thread-8 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877040000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877040000'
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877045000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877045000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877045000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877045000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877045000 ms.0 from job set of time 1542877045000 ms
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877045000 ms to writer queue
INFO pool-17-thread-9 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877045000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877045000'
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 4 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 8)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 8 (MapPartitionsRDD[18] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-9 org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.145.1:61945 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[18] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 6 tasks
INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 29, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 29)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 28)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 0 offsets 15095 -> 15135
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 5 offsets 15253 -> 15287
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 0 15095
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 5 15253
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=M]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=9999]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892495120622861]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=1109121500]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=V]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=XXXX]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892499617933046]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0711244300]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=R]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=1]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 8.0 (TID 29)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 8.0 (TID 28)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 8.0 (TID 30, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
WARN task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 8.0 (TID 29, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 2.0 in stage 8.0 (TID 30)
ERROR task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Task 1 in stage 8.0 failed 1 times; aborting job
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 8
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 1 offsets 25880 -> 25920
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 1 25880
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Stage 8 was cancelled
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 8 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) failed in 0.014 s due to Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 29, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 8.0 (TID 28) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 1]
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 4 failed: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.020485 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877045000 ms.0 from job set of time 1542877045000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.055 s for time 1542877045000 ms (execution: 0.026 s)
INFO JobGenerator org.apache.spark.rdd.ShuffledRDD - Removing RDD 15 from persistence list
ERROR JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Error running job streaming job 1542877045000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 8.0 failed 1 times, most recent failure: Lost task 1.0 in stage 8.0 (TID 29, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)
	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:144)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	... 3 more
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 14 from persistence list
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 15
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 14
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=B]
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 13 from persistence list
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892493659321581]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0705041000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO block-manager-slave-async-thread-pool-7 org.apache.spark.storage.BlockManager - Removing RDD 13
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO JobGenerator org.apache.spark.streaming.kafka010.KafkaRDD - Removing RDD 12 from persistence list
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 12
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877045000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877045000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877045000 ms
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 2.0 in stage 8.0 (TID 30)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877045000 ms to writer queue
INFO pool-17-thread-10 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877045000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877045000'
INFO dispatcher-event-loop-3 org.apache.spark.executor.Executor - Executor is trying to kill task 2.0 in stage 8.0 (TID 30)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Lost task 2.0 in stage 8.0 (TID 30) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 2]
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877050000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877050000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877050000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877050000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877050000 ms.0 from job set of time 1542877050000 ms
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877050000 ms to writer queue
INFO pool-17-thread-11 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877050000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877050000'
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 5 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 10)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-11 org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.145.1:61945 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 6 tasks
INFO dispatcher-event-loop-0 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 31, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-0 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 32, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 32)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 31)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 0 offsets 15135 -> 15155
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 0 15135
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 5 offsets 15287 -> 15304
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 5 15287
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=V]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=XXXX]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=M]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892499617933046]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0711244300]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=R]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=1]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=9999]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892495120622861]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=1109121500]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 10.0 (TID 31)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 10.0 (TID 32)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 10.0 (TID 33, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 2.0 in stage 10.0 (TID 33)
WARN task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 10.0 (TID 31, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 1 offsets 25920 -> 25940
ERROR task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 10.0 failed 1 times; aborting job
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 10.0 (TID 32) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 1]
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 1 25920
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 10
INFO dispatcher-event-loop-4 org.apache.spark.executor.Executor - Executor is trying to kill task 2.0 in stage 10.0 (TID 33)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Stage 10 was cancelled
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 10 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) failed in 0.010 s due to Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 31, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 5 failed: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.015324 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877050000 ms.0 from job set of time 1542877050000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.039 s for time 1542877050000 ms (execution: 0.020 s)
INFO JobGenerator org.apache.spark.rdd.ShuffledRDD - Removing RDD 19 from persistence list
ERROR JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Error running job streaming job 1542877050000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 10.0 failed 1 times, most recent failure: Lost task 0.0 in stage 10.0 (TID 31, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)
	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:144)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	... 3 more
INFO block-manager-slave-async-thread-pool-9 org.apache.spark.storage.BlockManager - Removing RDD 19
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 18 from persistence list
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=B]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892493659321581]
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 18
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0705041000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO JobGenerator org.apache.spark.rdd.MapPartitionsRDD - Removing RDD 17 from persistence list
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO block-manager-slave-async-thread-pool-9 org.apache.spark.storage.BlockManager - Removing RDD 17
INFO JobGenerator org.apache.spark.streaming.kafka010.KafkaRDD - Removing RDD 16 from persistence list
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 2.0 in stage 10.0 (TID 33)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Checkpointing graph for time 1542877050000 ms
INFO block-manager-slave-async-thread-pool-8 org.apache.spark.storage.BlockManager - Removing RDD 16
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updating checkpoint data for time 1542877050000 ms
INFO JobGenerator org.apache.spark.streaming.DStreamGraph - Updated checkpoint data for time 1542877050000 ms
INFO JobGenerator org.apache.spark.streaming.CheckpointWriter - Submitted checkpoint of time 1542877050000 ms to writer queue
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Lost task 2.0 in stage 10.0 (TID 33) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 2]
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
INFO pool-17-thread-12 org.apache.spark.streaming.CheckpointWriter - Saving checkpoint for time 1542877050000 ms to file 'file:/D:/streaming_checkpoint/checkpoint-1542877050000'
INFO main org.apache.spark.SparkContext - Running Spark version 2.1.0
INFO main org.apache.spark.SecurityManager - Changing view acls to: wulong
INFO main org.apache.spark.SecurityManager - Changing modify acls to: wulong
INFO main org.apache.spark.SecurityManager - Changing view acls groups to: 
INFO main org.apache.spark.SecurityManager - Changing modify acls groups to: 
INFO main org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wulong); groups with view permissions: Set(); users  with modify permissions: Set(wulong); groups with modify permissions: Set()
INFO main org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 62025.
INFO main org.apache.spark.SparkEnv - Registering MapOutputTracker
INFO main org.apache.spark.SparkEnv - Registering BlockManagerMaster
INFO main org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
INFO main org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
INFO main org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\wulong\AppData\Local\Temp\blockmgr-5a6ce17c-af04-42bc-9bf6-271df028783e
INFO main org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 4.1 GB
INFO main org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
INFO main org.spark_project.jetty.util.log - Logging initialized @1398ms
INFO main org.spark_project.jetty.server.Server - jetty-9.2.z-SNAPSHOT
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@482d776b{/jobs,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4052274f{/jobs/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@132ddbab{/jobs/job,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@297ea53a{/jobs/job/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@acb0951{/stages,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf22f18{/stages/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@267f474e{/stages/stage,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a7471ce{/stages/stage/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28276e50{/stages/pool,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e70ea3{/stages/pool/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3efe7086{/storage,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@675d8c96{/storage/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@741b3bc3{/storage/rdd,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/storage/rdd/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63648ee9{/environment,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d6972f{/environment/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45be7cd5{/executors,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7651218e{/executors/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3185fa6b{/executors/threadDump,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d366c9b{/executors/threadDump/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b58ed3c{/static,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24faea88{/,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a320ade{/api,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64beebb7{/jobs/job/kill,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7813cb11{/stages/stage/kill,null,AVAILABLE}
INFO main org.spark_project.jetty.server.ServerConnector - Started ServerConnector@64337702{HTTP/1.1}{0.0.0.0:4040}
INFO main org.spark_project.jetty.server.Server - Started @1480ms
INFO main org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
INFO main org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
INFO main org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
INFO main org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62042.
INFO main org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:62042
INFO main org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
INFO main org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 62042, None)
INFO dispatcher-event-loop-10 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:62042 with 4.1 GB RAM, BlockManagerId(driver, 192.168.145.1, 62042, None)
INFO main org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 62042, None)
INFO main org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 62042, None)
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/metrics/json,null,AVAILABLE}
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding enable.auto.commit to false for executor
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding auto.offset.reset to none for executor
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding executor group.id to spark-executor-consumer_test
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding receive.buffer.bytes to 65536 see KAFKA-3135
INFO main com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - 获取到流对象...............
INFO main com.hxqh.bigdata.parse.Flow_InitParseRuleMapping_Job - 初始化码表映射..................
INFO main com.hxqh.bigdata.parse.Flow_InitParseRuleMapping_Job - Hubble 数据库已连接..................
INFO main com.hxqh.bigdata.parse.Flow_InitParseRuleMapping_Job - 需要解析的指标及规则 : {CHANNEL_NO=MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=null], TRANS_TYPE=MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=null], EMAIL=MsgParser [name=EMAIL, length=40, start=100, end=140, value=null], C1=MsgParser [name=C1, length=1, start=220, end=221, value=null], C2=MsgParser [name=C2, length=2, start=221, end=223, value=null], AMT_SIGN=MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value=null], CARD_NBR=MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=null], MB_HONE=MsgParser [name=MB_HONE, length=12, start=58, end=70, value=null], AMOUNT=MsgParser [name=AMOUNT, length=12, start=42, end=54, value=null], TRANS_CODE=MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=null], DATE_ME=MsgParser [name=DATE_ME, length=10, start=32, end=42, value=null], OR_FLG=MsgParser [name=OR_FLG, length=1, start=31, end=32, value=null], YSQ_TRANS_TYPE=MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=null], BNK_NBR=MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=null], CURR_NUM=MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=null], TRANS_CHANNEL_TYPE_DEF=MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=null], TRANS_ORG_CODE=MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=null], MCC=MsgParser [name=MCC, length=4, start=199, end=203, value=null], CN_AMOUNT=MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=null], NAME=MsgParser [name=NAME, length=30, start=70, end=100, value=null], REVADDRESS=MsgParser [name=REVADDRESS, length=40, start=223, end=263, value=null], SEC_RESP_CODE=MsgParser [name=SEC_RESP_CODE, length=8, start=141, end=149, value=null], PAY_AMOUNT=MsgParser [name=PAY_AMOUNT, length=12, start=187, end=199, value=null], ACCOUNT_LIMIT=MsgParser [name=ACCOUNT_LIMIT, length=10, start=149, end=159, value=null], COMMERCIAL_CODE=MsgParser [name=COMMERCIAL_CODE, length=15, start=159, end=174, value=null], POS_INP=MsgParser [name=POS_INP, length=4, start=203, end=207, value=null]}
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@1fc713c9
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@70c69586
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@7c0777b5
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@5e5aafc6
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5d1e0fbb
INFO ForkJoinPool-2-worker-29 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

INFO ForkJoinPool-2-worker-29 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

WARN ForkJoinPool-2-worker-29 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO ForkJoinPool-2-worker-29 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO ForkJoinPool-2-worker-29 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO ForkJoinPool-2-worker-29 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node3:9092 (id: 2147483645 rack: null) for group consumer_test.
INFO ForkJoinPool-2-worker-29 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [] for group consumer_test
INFO ForkJoinPool-2-worker-29 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - (Re-)joining group consumer_test
INFO main org.apache.spark.SparkContext - Running Spark version 2.1.0
INFO main org.apache.spark.SecurityManager - Changing view acls to: wulong
INFO main org.apache.spark.SecurityManager - Changing modify acls to: wulong
INFO main org.apache.spark.SecurityManager - Changing view acls groups to: 
INFO main org.apache.spark.SecurityManager - Changing modify acls groups to: 
INFO main org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(wulong); groups with view permissions: Set(); users  with modify permissions: Set(wulong); groups with modify permissions: Set()
INFO main org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 62089.
INFO main org.apache.spark.SparkEnv - Registering MapOutputTracker
INFO main org.apache.spark.SparkEnv - Registering BlockManagerMaster
INFO main org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
INFO main org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
INFO main org.apache.spark.storage.DiskBlockManager - Created local directory at C:\Users\wulong\AppData\Local\Temp\blockmgr-521a4bd0-1a18-4d16-b550-e756f4222b10
INFO main org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 4.1 GB
INFO main org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
INFO main org.spark_project.jetty.util.log - Logging initialized @1386ms
INFO main org.spark_project.jetty.server.Server - jetty-9.2.z-SNAPSHOT
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@482d776b{/jobs,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4052274f{/jobs/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@132ddbab{/jobs/job,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@297ea53a{/jobs/job/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@acb0951{/stages,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5bf22f18{/stages/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@267f474e{/stages/stage,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7a7471ce{/stages/stage/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@28276e50{/stages/pool,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@62e70ea3{/stages/pool/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3efe7086{/storage,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@675d8c96{/storage/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@741b3bc3{/storage/rdd,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2ed3b1f5{/storage/rdd/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@63648ee9{/environment,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@68d6972f{/environment/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@45be7cd5{/executors,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7651218e{/executors/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3185fa6b{/executors/threadDump,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6d366c9b{/executors/threadDump/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5b58ed3c{/static,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24faea88{/,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3a320ade{/api,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@64beebb7{/jobs/job/kill,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7813cb11{/stages/stage/kill,null,AVAILABLE}
INFO main org.spark_project.jetty.server.ServerConnector - Started ServerConnector@64337702{HTTP/1.1}{0.0.0.0:4040}
INFO main org.spark_project.jetty.server.Server - Started @1469ms
INFO main org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
INFO main org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://192.168.145.1:4040
INFO main org.apache.spark.executor.Executor - Starting executor ID driver on host localhost
INFO main org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62106.
INFO main org.apache.spark.network.netty.NettyBlockTransferService - Server created on 192.168.145.1:62106
INFO main org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
INFO main org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, 192.168.145.1, 62106, None)
INFO dispatcher-event-loop-10 org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager 192.168.145.1:62106 with 4.1 GB RAM, BlockManagerId(driver, 192.168.145.1, 62106, None)
INFO main org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, 192.168.145.1, 62106, None)
INFO main org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, 192.168.145.1, 62106, None)
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@26f3d90c{/metrics/json,null,AVAILABLE}
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding enable.auto.commit to false for executor
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding auto.offset.reset to none for executor
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding executor group.id to spark-executor-consumer_test
WARN main org.apache.spark.streaming.kafka010.KafkaUtils - overriding receive.buffer.bytes to 65536 see KAFKA-3135
INFO main com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - 获取到流对象...............
INFO main com.hxqh.bigdata.parse.Flow_InitParseRuleMapping_Job - 初始化码表映射..................
INFO main com.hxqh.bigdata.parse.Flow_InitParseRuleMapping_Job - Hubble 数据库已连接..................
INFO main com.hxqh.bigdata.parse.Flow_InitParseRuleMapping_Job - 需要解析的指标及规则 : {CHANNEL_NO=MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=null], TRANS_TYPE=MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=null], EMAIL=MsgParser [name=EMAIL, length=40, start=100, end=140, value=null], C1=MsgParser [name=C1, length=1, start=220, end=221, value=null], C2=MsgParser [name=C2, length=2, start=221, end=223, value=null], AMT_SIGN=MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value=null], CARD_NBR=MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=null], MB_HONE=MsgParser [name=MB_HONE, length=12, start=58, end=70, value=null], AMOUNT=MsgParser [name=AMOUNT, length=12, start=42, end=54, value=null], TRANS_CODE=MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=null], DATE_ME=MsgParser [name=DATE_ME, length=10, start=32, end=42, value=null], OR_FLG=MsgParser [name=OR_FLG, length=1, start=31, end=32, value=null], YSQ_TRANS_TYPE=MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=null], BNK_NBR=MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=null], CURR_NUM=MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=null], TRANS_CHANNEL_TYPE_DEF=MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=null], TRANS_ORG_CODE=MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=null], MCC=MsgParser [name=MCC, length=4, start=199, end=203, value=null], CN_AMOUNT=MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=null], NAME=MsgParser [name=NAME, length=30, start=70, end=100, value=null], REVADDRESS=MsgParser [name=REVADDRESS, length=40, start=223, end=263, value=null], SEC_RESP_CODE=MsgParser [name=SEC_RESP_CODE, length=8, start=141, end=149, value=null], PAY_AMOUNT=MsgParser [name=PAY_AMOUNT, length=12, start=187, end=199, value=null], ACCOUNT_LIMIT=MsgParser [name=ACCOUNT_LIMIT, length=10, start=149, end=159, value=null], COMMERCIAL_CODE=MsgParser [name=COMMERCIAL_CODE, length=15, start=159, end=174, value=null], POS_INP=MsgParser [name=POS_INP, length=4, start=203, end=207, value=null]}
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.kafka010.DirectKafkaInputDStream - Initialized and validated org.apache.spark.streaming.kafka010.DirectKafkaInputDStream@6e454653
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@1ba121f
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.MappedDStream - Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@4ae69147
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ShuffledDStream - Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@34c1aa9d
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Slide time = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Storage level = Serialized 1x Replicated
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Checkpoint interval = null
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Remember interval = 5000 ms
INFO streaming-start org.apache.spark.streaming.dstream.ForEachDStream - Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@70f20588
INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = true
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-1
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = latest

WARN ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO ForkJoinPool-1-worker-29 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO ForkJoinPool-1-worker-29 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node3:9092 (id: 2147483645 rack: null) for group consumer_test.
INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Revoking previously assigned partitions [] for group consumer_test
INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - (Re-)joining group consumer_test
INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Successfully joined group consumer_test with generation 4
INFO ForkJoinPool-1-worker-29 org.apache.kafka.clients.consumer.internals.ConsumerCoordinator - Setting newly assigned partitions [test-1, test-0, test-3, test-2, test-5, test-4] for group consumer_test
INFO streaming-start org.apache.spark.streaming.util.RecurringTimer - Started timer for JobGenerator at time 1542877085000
INFO streaming-start org.apache.spark.streaming.scheduler.JobGenerator - Started JobGenerator at 1542877085000 ms
INFO streaming-start org.apache.spark.streaming.scheduler.JobScheduler - Started JobScheduler
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@2bfb6b49{/streaming,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@46a145ba{/streaming/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1a1f5f71{/streaming/batch,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@524270b8{/streaming/batch/json,null,AVAILABLE}
INFO main org.spark_project.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@29149030{/static/streaming,null,AVAILABLE}
INFO main org.apache.spark.streaming.StreamingContext - StreamingContext started
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877085000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877085000 ms.0 from job set of time 1542877085000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877090000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877095000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877100000 ms
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877105000 ms
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 2 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1542877110000 ms
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 0 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 0)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 0)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-10 org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on 192.168.145.1:62106 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 6 tasks
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 6207 bytes)
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 0.0 (TID 1)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 5 offsets 15287 -> 15712
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 0 offsets 15135 -> 15635
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initializing cache 16 64 0.75
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,5)
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-2
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

WARN Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,0)
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-3
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

WARN Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 5 15287
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 0 15135
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=M]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=9999]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=V]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=XXXX]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892499617933046]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892495120622861]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=1109121500]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0711244300]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=R]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=1]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 1.0 in stage 0.0 (TID 1)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 0.0 (TID 2, localhost, executor driver, partition 2, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 2.0 in stage 0.0 (TID 2)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 0.0 (TID 3, localhost, executor driver, partition 3, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 3.0 in stage 0.0 (TID 3)
WARN task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 1 offsets 25920 -> 26420
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,1)
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

ERROR task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Computing topic test, partition 4 offsets 31280 -> 31530
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Lost task 1.0 in stage 0.0 (TID 1) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 1]
INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-4
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

WARN Executor task launch worker-1 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO Executor task launch worker-1 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Cache miss for CacheKey(spark-executor-consumer_test,test,4)
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = 
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 1 25920
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Cancelling stage 0
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - ConsumerConfig values: 
	metric.reporters = []
	metadata.max.age.ms = 300000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	reconnect.backoff.ms = 50
	sasl.kerberos.ticket.renew.window.factor = 0.8
	max.partition.fetch.bytes = 1048576
	bootstrap.servers = [192.168.145.101:9092, 192.168.145.102:9092, 192.168.145.103:9092]
	ssl.keystore.type = JKS
	enable.auto.commit = false
	sasl.mechanism = GSSAPI
	interceptor.classes = null
	exclude.internal.topics = true
	ssl.truststore.password = null
	client.id = consumer-5
	ssl.endpoint.identification.algorithm = null
	max.poll.records = 2147483647
	check.crcs = true
	request.timeout.ms = 40000
	heartbeat.interval.ms = 3000
	auto.commit.interval.ms = 5000
	receive.buffer.bytes = 65536
	ssl.truststore.type = JKS
	ssl.truststore.location = null
	ssl.keystore.password = null
	fetch.min.bytes = 1
	send.buffer.bytes = 131072
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	group.id = spark-executor-consumer_test
	retry.backoff.ms = 100
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	ssl.trustmanager.algorithm = PKIX
	ssl.key.password = null
	fetch.max.wait.ms = 500
	sasl.kerberos.min.time.before.relogin = 60000
	connections.max.idle.ms = 540000
	session.timeout.ms = 30000
	metrics.num.samples = 2
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	ssl.protocol = TLS
	ssl.provider = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.keystore.location = null
	ssl.cipher.suites = null
	security.protocol = PLAINTEXT
	ssl.keymanager.algorithm = SunX509
	metrics.sample.window.ms = 30000
	auto.offset.reset = none

INFO Executor task launch worker-1 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
WARN Executor task launch worker-0 org.apache.kafka.clients.consumer.ConsumerConfig - The configuration metadata.broker.list = 192.168.145.101:9092,192.168.145.102:9092,192.168.145.103:9092 was supplied but isn't a known config.
INFO dispatcher-event-loop-6 org.apache.spark.executor.Executor - Executor is trying to kill task 2.0 in stage 0.0 (TID 2)
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka version : 0.10.0.1
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Stage 0 was cancelled
INFO Executor task launch worker-0 org.apache.kafka.common.utils.AppInfoParser - Kafka commitId : a7a17cdec9eaa6c5
INFO dispatcher-event-loop-6 org.apache.spark.executor.Executor - Executor is trying to kill task 3.0 in stage 0.0 (TID 3)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.CachedKafkaConsumer - Initial fetch for spark-executor-consumer_test test 4 31280
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 0 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) failed in 0.179 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
INFO Executor task launch worker-0 org.apache.kafka.clients.consumer.internals.AbstractCoordinator - Discovered coordinator node2:9092 (id: 2147483646 rack: null) for group spark-executor-consumer_test.
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 0 failed: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.314695 s
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=B]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892493659321581]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0705041000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=2]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-1 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
ERROR Executor task launch worker-1 org.apache.spark.executor.Executor - Exception in task 2.0 in stage 0.0 (TID 2)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877085000 ms.0 from job set of time 1542877085000 ms
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CHANNEL_NO, length=1, start=174, end=175, value=I]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_TYPE, length=4, start=27, end=31, value=1050]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=EMAIL, length=40, start=100, end=140, value=                                        ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C1, length=1, start=220, end=221, value=0]
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 25.457 s for time 1542877085000 ms (execution: 0.361 s)
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=C2, length=2, start=221, end=223, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMT_SIGN, length=1, start=54, end=55, value= ]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CARD_NBR, length=19, start=8, end=27, value=4512892493626624894]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MB_HONE, length=12, start=58, end=70, value=000000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=AMOUNT, length=12, start=42, end=54, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CODE, length=4, start=0, end=4, value=3001]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=DATE_ME, length=10, start=32, end=42, value=0624095600]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=OR_FLG, length=1, start=31, end=32, value=O]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=YSQ_TRANS_TYPE, length=1, start=140, end=141, value=3]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=BNK_NBR, length=4, start=4, end=8, value=0309]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CURR_NUM, length=3, start=55, end=58, value=155]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_CHANNEL_TYPE_DEF, length=2, start=218, end=220, value=00]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=TRANS_ORG_CODE, length=11, start=207, end=218, value=00000000000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=MCC, length=4, start=199, end=203, value=0000]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=CN_AMOUNT, length=12, start=175, end=187, value=000000000010]
INFO Executor task launch worker-0 com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job - MsgParser [name=NAME, length=30, start=70, end=100, value=                              ]
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877090000 ms.0 from job set of time 1542877090000 ms
ERROR Executor task launch worker-0 org.apache.spark.executor.Executor - Exception in task 3.0 in stage 0.0 (TID 3)
java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
ERROR JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Error running job streaming job 1542877085000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:934)
	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361)
	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:144)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$4.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.api.java.JavaDStreamLike$$anonfun$foreachRDD$1.apply(JavaDStreamLike.scala:272)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:627)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:51)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:50)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:254)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:253)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 263
	at java.lang.String.substring(String.java:1963)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:109)
	at com.hxqh.bigdata.spark.Flow_DashBoardRealTimeOp_Job$1.call(Flow_DashBoardRealTimeOp_Job.java:1)
	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:193)
	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	... 3 more
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Lost task 2.0 in stage 0.0 (TID 2) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 2]
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Lost task 3.0 in stage 0.0 (TID 3) on localhost, executor driver: java.lang.StringIndexOutOfBoundsException (String index out of range: 263) [duplicate 3]
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 6 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 3 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 2)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 2)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[6] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-5 org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on 192.168.145.1:62106 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[6] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 6 tasks
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, PROCESS_LOCAL, 6207 bytes)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 2.0 (TID 5)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 4)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15712 is the same as ending offset skipping test 5
INFO Thread-1 org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15635 is the same as ending offset skipping test 0
INFO JobGenerator org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Deleting batches: 
INFO Thread-1 org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker stopped
INFO Thread-1 org.apache.spark.streaming.scheduler.JobGenerator - Stopping JobGenerator immediately
INFO Thread-1 org.apache.spark.streaming.util.RecurringTimer - Stopped timer for JobGenerator after time 1542877110000
INFO JobGenerator org.apache.spark.streaming.scheduler.InputInfoTracker - remove old batch metadata: 
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 4). 1235 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 1.0 in stage 2.0 (TID 5). 1235 bytes result sent to driver
INFO dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 6, localhost, executor driver, partition 2, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 2.0 in stage 2.0 (TID 6)
INFO dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 7, localhost, executor driver, partition 3, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 3.0 in stage 2.0 (TID 7)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26420 is the same as ending offset skipping test 1
INFO Thread-1 org.apache.spark.streaming.scheduler.JobGenerator - Stopped JobGenerator
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 31530 is the same as ending offset skipping test 4
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 5) in 109 ms on localhost (executor driver) (1/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 4) in 112 ms on localhost (executor driver) (2/6)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 2.0 in stage 2.0 (TID 6). 1148 bytes result sent to driver
INFO dispatcher-event-loop-1 org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 8, localhost, executor driver, partition 4, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 4.0 in stage 2.0 (TID 8)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 6) in 10 ms on localhost (executor driver) (3/6)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 3.0 in stage 2.0 (TID 7). 1148 bytes result sent to driver
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 9, localhost, executor driver, partition 5, PROCESS_LOCAL, 6207 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 5.0 in stage 2.0 (TID 9)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 7) in 10 ms on localhost (executor driver) (4/6)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 27142 is the same as ending offset skipping test 3
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26009 is the same as ending offset skipping test 2
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 4.0 in stage 2.0 (TID 8). 1148 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 5.0 in stage 2.0 (TID 9). 1148 bytes result sent to driver
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 8) in 9 ms on localhost (executor driver) (5/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 9) in 8 ms on localhost (executor driver) (6/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) finished in 0.125 s
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - running: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 3)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - failed: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 3 (ShuffledRDD[7] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 3.1 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 1865.0 B, free 4.1 GB)
INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on 192.168.145.1:62106 (size: 1865.0 B, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 3 (ShuffledRDD[7] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 2 tasks
INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 10, localhost, executor driver, partition 0, PROCESS_LOCAL, 6137 bytes)
INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 3.0 (TID 11, localhost, executor driver, partition 1, PROCESS_LOCAL, 6137 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 3.0 (TID 11)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 10)
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 4 ms
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 3 ms
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 10). 1718 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 1.0 in stage 3.0 (TID 11). 1639 bytes result sent to driver
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 10) in 19 ms on localhost (executor driver) (1/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 3.0 (TID 11) in 18 ms on localhost (executor driver) (2/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 3 (collect at Flow_DashBoardRealTimeOp_Job.java:144) finished in 0.020 s
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.169098 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877090000 ms.0 from job set of time 1542877090000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 20.642 s for time 1542877090000 ms (execution: 0.179 s)
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877095000 ms.0 from job set of time 1542877095000 ms
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 2 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 4)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 4)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 4 (MapPartitionsRDD[10] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-6 org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on 192.168.145.1:62106 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 3 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[10] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 6 tasks
INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 12, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-7 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 4.0 (TID 13, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 1.0 in stage 4.0 (TID 13)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 12)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15712 is the same as ending offset skipping test 5
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15635 is the same as ending offset skipping test 0
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 1.0 in stage 4.0 (TID 13). 1148 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 12). 1148 bytes result sent to driver
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 4.0 (TID 14, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 2.0 in stage 4.0 (TID 14)
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 4.0 (TID 15, localhost, executor driver, partition 3, PROCESS_LOCAL, 6208 bytes)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 4.0 (TID 13) in 7 ms on localhost (executor driver) (1/6)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 3.0 in stage 4.0 (TID 15)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26420 is the same as ending offset skipping test 1
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 12) in 9 ms on localhost (executor driver) (2/6)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 31530 is the same as ending offset skipping test 4
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 2.0 in stage 4.0 (TID 14). 1148 bytes result sent to driver
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 4.0 (TID 16, localhost, executor driver, partition 4, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 4.0 in stage 4.0 (TID 16)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 4.0 (TID 14) in 6 ms on localhost (executor driver) (3/6)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 3.0 in stage 4.0 (TID 15). 1227 bytes result sent to driver
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 4.0 (TID 17, localhost, executor driver, partition 5, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 5.0 in stage 4.0 (TID 17)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 4.0 (TID 15) in 8 ms on localhost (executor driver) (4/6)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 27142 is the same as ending offset skipping test 3
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26009 is the same as ending offset skipping test 2
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 4.0 in stage 4.0 (TID 16). 1235 bytes result sent to driver
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 4.0 (TID 16) in 6 ms on localhost (executor driver) (5/6)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 5.0 in stage 4.0 (TID 17). 1148 bytes result sent to driver
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 4.0 (TID 17) in 6 ms on localhost (executor driver) (6/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 4 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) finished in 0.021 s
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - running: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 5)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - failed: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (ShuffledRDD[11] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 3.1 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 1862.0 B, free 4.1 GB)
INFO dispatcher-event-loop-9 org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on 192.168.145.1:62106 (size: 1862.0 B, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 5 (ShuffledRDD[11] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 2 tasks
INFO dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 18, localhost, executor driver, partition 0, PROCESS_LOCAL, 6138 bytes)
INFO dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 19, localhost, executor driver, partition 1, PROCESS_LOCAL, 6138 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 5.0 (TID 19)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 18)
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 18). 1639 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 1.0 in stage 5.0 (TID 19). 1639 bytes result sent to driver
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 18) in 7 ms on localhost (executor driver) (1/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 19) in 5 ms on localhost (executor driver) (2/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (collect at Flow_DashBoardRealTimeOp_Job.java:144) finished in 0.007 s
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 2 finished: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.040402 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877095000 ms.0 from job set of time 1542877095000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 15.687 s for time 1542877095000 ms (execution: 0.045 s)
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877100000 ms.0 from job set of time 1542877100000 ms
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 3 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 7 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 6)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 6)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 6 (MapPartitionsRDD[14] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-9 org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on 192.168.145.1:62106 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[14] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 6.0 with 6 tasks
INFO dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 20, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-10 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 6.0 (TID 21, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 1.0 in stage 6.0 (TID 21)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 0.0 in stage 6.0 (TID 20)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15712 is the same as ending offset skipping test 5
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15635 is the same as ending offset skipping test 0
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 0.0 in stage 6.0 (TID 20). 1148 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 1.0 in stage 6.0 (TID 21). 1148 bytes result sent to driver
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 6.0 (TID 22, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 2.0 in stage 6.0 (TID 22)
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 6.0 (TID 23, localhost, executor driver, partition 3, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 3.0 in stage 6.0 (TID 23)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 20) in 9 ms on localhost (executor driver) (1/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 6.0 (TID 21) in 8 ms on localhost (executor driver) (2/6)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26420 is the same as ending offset skipping test 1
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 31530 is the same as ending offset skipping test 4
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 2.0 in stage 6.0 (TID 22). 1148 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 3.0 in stage 6.0 (TID 23). 1148 bytes result sent to driver
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 6.0 (TID 24, localhost, executor driver, partition 4, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 4.0 in stage 6.0 (TID 24)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 6.0 (TID 25, localhost, executor driver, partition 5, PROCESS_LOCAL, 6208 bytes)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 6.0 (TID 22) in 7 ms on localhost (executor driver) (3/6)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 5.0 in stage 6.0 (TID 25)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 6.0 (TID 23) in 7 ms on localhost (executor driver) (4/6)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 27142 is the same as ending offset skipping test 3
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26009 is the same as ending offset skipping test 2
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 4.0 in stage 6.0 (TID 24). 1238 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 5.0 in stage 6.0 (TID 25). 1235 bytes result sent to driver
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 6.0 (TID 24) in 6 ms on localhost (executor driver) (5/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 6.0 (TID 25) in 6 ms on localhost (executor driver) (6/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 6.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 6 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) finished in 0.020 s
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - running: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 7)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - failed: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 7 (ShuffledRDD[15] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 3.1 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 1867.0 B, free 4.1 GB)
INFO dispatcher-event-loop-1 org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on 192.168.145.1:62106 (size: 1867.0 B, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 6 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 7 (ShuffledRDD[15] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 7.0 with 2 tasks
INFO dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 26, localhost, executor driver, partition 0, PROCESS_LOCAL, 6138 bytes)
INFO dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 27, localhost, executor driver, partition 1, PROCESS_LOCAL, 6138 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 7.0 (TID 27)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 7.0 (TID 26)
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 7.0 (TID 26). 1628 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 1.0 in stage 7.0 (TID 27). 1628 bytes result sent to driver
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 7.0 (TID 26) in 5 ms on localhost (executor driver) (1/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 7.0 (TID 27) in 4 ms on localhost (executor driver) (2/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 7.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 7 (collect at Flow_DashBoardRealTimeOp_Job.java:144) finished in 0.005 s
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 3 finished: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.037279 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877100000 ms.0 from job set of time 1542877100000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 10.732 s for time 1542877100000 ms (execution: 0.045 s)
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877105000 ms.0 from job set of time 1542877105000 ms
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 4 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 9 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 8)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 8)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 8 (MapPartitionsRDD[18] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-1 org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on 192.168.145.1:62106 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[18] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 8.0 with 6 tasks
INFO dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 8.0 (TID 28, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-8 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 8.0 (TID 29, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 1.0 in stage 8.0 (TID 29)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 0.0 in stage 8.0 (TID 28)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15635 is the same as ending offset skipping test 0
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15712 is the same as ending offset skipping test 5
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 1.0 in stage 8.0 (TID 29). 1238 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 0.0 in stage 8.0 (TID 28). 1238 bytes result sent to driver
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 8.0 (TID 30, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 2.0 in stage 8.0 (TID 30)
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 8.0 (TID 31, localhost, executor driver, partition 3, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 3.0 in stage 8.0 (TID 31)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 8.0 (TID 29) in 7 ms on localhost (executor driver) (1/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 8.0 (TID 28) in 8 ms on localhost (executor driver) (2/6)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26420 is the same as ending offset skipping test 1
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 31530 is the same as ending offset skipping test 4
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 2.0 in stage 8.0 (TID 30). 1148 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 3.0 in stage 8.0 (TID 31). 1148 bytes result sent to driver
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 8.0 (TID 32, localhost, executor driver, partition 4, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 4.0 in stage 8.0 (TID 32)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 8.0 (TID 30) in 7 ms on localhost (executor driver) (3/6)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 8.0 (TID 33, localhost, executor driver, partition 5, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 5.0 in stage 8.0 (TID 33)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 8.0 (TID 31) in 7 ms on localhost (executor driver) (4/6)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 27142 is the same as ending offset skipping test 3
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26009 is the same as ending offset skipping test 2
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 4.0 in stage 8.0 (TID 32). 1148 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 5.0 in stage 8.0 (TID 33). 1227 bytes result sent to driver
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 8.0 (TID 32) in 6 ms on localhost (executor driver) (5/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 8.0 (TID 33) in 6 ms on localhost (executor driver) (6/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 8.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 8 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) finished in 0.020 s
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - running: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 9)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - failed: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 9 (ShuffledRDD[19] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 3.1 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 1867.0 B, free 4.1 GB)
INFO dispatcher-event-loop-0 org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on 192.168.145.1:62106 (size: 1867.0 B, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 9 (ShuffledRDD[19] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 9.0 with 2 tasks
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 9.0 (TID 34, localhost, executor driver, partition 0, PROCESS_LOCAL, 6138 bytes)
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 9.0 (TID 35, localhost, executor driver, partition 1, PROCESS_LOCAL, 6138 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 1.0 in stage 9.0 (TID 35)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 0.0 in stage 9.0 (TID 34)
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 1 ms
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 0.0 in stage 9.0 (TID 34). 1549 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 1.0 in stage 9.0 (TID 35). 1549 bytes result sent to driver
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 9.0 (TID 34) in 4 ms on localhost (executor driver) (1/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 9.0 (TID 35) in 3 ms on localhost (executor driver) (2/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 9.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 9 (collect at Flow_DashBoardRealTimeOp_Job.java:144) finished in 0.005 s
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 4 finished: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.035018 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877105000 ms.0 from job set of time 1542877105000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 5.773 s for time 1542877105000 ms (execution: 0.041 s)
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Starting job streaming job 1542877110000 ms.0 from job set of time 1542877110000 ms
INFO streaming-job-executor-0 org.apache.spark.SparkContext - Starting job: collect at Flow_DashBoardRealTimeOp_Job.java:144
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Got job 5 (collect at Flow_DashBoardRealTimeOp_Job.java:144) with 2 output partitions
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 11 (collect at Flow_DashBoardRealTimeOp_Job.java:144)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 10)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 10)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 10 (MapPartitionsRDD[22] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 5.8 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 3.5 KB, free 4.1 GB)
INFO dispatcher-event-loop-0 org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on 192.168.145.1:62106 (size: 3.5 KB, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 9 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 6 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[22] at mapToPair at Flow_DashBoardRealTimeOp_Job.java:122)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 10.0 with 6 tasks
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 10.0 (TID 36, localhost, executor driver, partition 0, PROCESS_LOCAL, 6208 bytes)
INFO dispatcher-event-loop-3 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 10.0 (TID 37, localhost, executor driver, partition 1, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 10.0 (TID 36)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 10.0 (TID 37)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15635 is the same as ending offset skipping test 0
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 15712 is the same as ending offset skipping test 5
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 1.0 in stage 10.0 (TID 37). 1148 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 10.0 (TID 36). 1061 bytes result sent to driver
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 10.0 (TID 38, localhost, executor driver, partition 2, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 2.0 in stage 10.0 (TID 38)
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 10.0 (TID 37) in 6 ms on localhost (executor driver) (1/6)
INFO dispatcher-event-loop-11 org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 10.0 (TID 39, localhost, executor driver, partition 3, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 3.0 in stage 10.0 (TID 39)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 10.0 (TID 36) in 8 ms on localhost (executor driver) (2/6)
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26420 is the same as ending offset skipping test 1
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 31530 is the same as ending offset skipping test 4
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 2.0 in stage 10.0 (TID 38). 1314 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 3.0 in stage 10.0 (TID 39). 1227 bytes result sent to driver
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 10.0 (TID 40, localhost, executor driver, partition 4, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 4.0 in stage 10.0 (TID 40)
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 10.0 (TID 38) in 7 ms on localhost (executor driver) (3/6)
INFO dispatcher-event-loop-2 org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 10.0 (TID 41, localhost, executor driver, partition 5, PROCESS_LOCAL, 6208 bytes)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 5.0 in stage 10.0 (TID 41)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 10.0 (TID 39) in 6 ms on localhost (executor driver) (4/6)
INFO Executor task launch worker-1 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 27142 is the same as ending offset skipping test 3
INFO Executor task launch worker-0 org.apache.spark.streaming.kafka010.KafkaRDD - Beginning offset 26009 is the same as ending offset skipping test 2
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 4.0 in stage 10.0 (TID 40). 1148 bytes result sent to driver
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 5.0 in stage 10.0 (TID 41). 1148 bytes result sent to driver
INFO task-result-getter-1 org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 10.0 (TID 40) in 6 ms on localhost (executor driver) (5/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 10.0 (TID 41) in 5 ms on localhost (executor driver) (6/6)
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 10.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 10 (mapToPair at Flow_DashBoardRealTimeOp_Job.java:122) finished in 0.018 s
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - running: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 11)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - failed: Set()
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 11 (ShuffledRDD[23] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133), which has no missing parents
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 3.1 KB, free 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 1864.0 B, free 4.1 GB)
INFO dispatcher-event-loop-5 org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on 192.168.145.1:62106 (size: 1864.0 B, free: 4.1 GB)
INFO dag-scheduler-event-loop org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:996
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - Submitting 2 missing tasks from ResultStage 11 (ShuffledRDD[23] at reduceByKey at Flow_DashBoardRealTimeOp_Job.java:133)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 11.0 with 2 tasks
INFO dispatcher-event-loop-4 org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 11.0 (TID 42, localhost, executor driver, partition 0, PROCESS_LOCAL, 6138 bytes)
INFO dispatcher-event-loop-4 org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 11.0 (TID 43, localhost, executor driver, partition 1, PROCESS_LOCAL, 6138 bytes)
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Running task 1.0 in stage 11.0 (TID 43)
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Running task 0.0 in stage 11.0 (TID 42)
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 0 non-empty blocks out of 6 blocks
INFO Executor task launch worker-0 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
INFO Executor task launch worker-1 org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 0 ms
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 11.0 (TID 42). 1628 bytes result sent to driver
INFO Executor task launch worker-1 org.apache.spark.executor.Executor - Finished task 1.0 in stage 11.0 (TID 43). 1628 bytes result sent to driver
INFO task-result-getter-2 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 11.0 (TID 42) in 6 ms on localhost (executor driver) (1/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 11.0 (TID 43) in 5 ms on localhost (executor driver) (2/2)
INFO task-result-getter-3 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 11.0, whose tasks have all completed, from pool 
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 11 (collect at Flow_DashBoardRealTimeOp_Job.java:144) finished in 0.006 s
INFO streaming-job-executor-0 org.apache.spark.scheduler.DAGScheduler - Job 5 finished: collect at Flow_DashBoardRealTimeOp_Job.java:144, took 0.034948 s
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Finished job streaming job 1542877110000 ms.0 from job set of time 1542877110000 ms
INFO JobScheduler org.apache.spark.streaming.scheduler.JobScheduler - Total delay: 0.812 s for time 1542877110000 ms (execution: 0.039 s)
INFO Thread-1 org.apache.spark.streaming.scheduler.JobScheduler - Stopped JobScheduler
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2bfb6b49{/streaming,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@1a1f5f71{/streaming/batch,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@29149030{/static/streaming,null,UNAVAILABLE}
INFO Thread-1 org.apache.spark.streaming.StreamingContext - StreamingContext stopped successfully
INFO Thread-1 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
INFO Thread-1 org.spark_project.jetty.server.ServerConnector - Stopped ServerConnector@64337702{HTTP/1.1}{0.0.0.0:4040}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7813cb11{/stages/stage/kill,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@64beebb7{/jobs/job/kill,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3a320ade{/api,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@24faea88{/,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5b58ed3c{/static,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@6d366c9b{/executors/threadDump/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3185fa6b{/executors/threadDump,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7651218e{/executors/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@45be7cd5{/executors,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@68d6972f{/environment/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@63648ee9{/environment,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@2ed3b1f5{/storage/rdd/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@741b3bc3{/storage/rdd,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@675d8c96{/storage/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@3efe7086{/storage,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@62e70ea3{/stages/pool/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@28276e50{/stages/pool,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@7a7471ce{/stages/stage/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@267f474e{/stages/stage,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@5bf22f18{/stages/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@acb0951{/stages,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@297ea53a{/jobs/job/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@132ddbab{/jobs/job,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@4052274f{/jobs/json,null,UNAVAILABLE}
INFO Thread-1 org.spark_project.jetty.server.handler.ContextHandler - Stopped o.s.j.s.ServletContextHandler@482d776b{/jobs,null,UNAVAILABLE}
INFO Thread-1 org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://192.168.145.1:4040
INFO dispatcher-event-loop-2 org.apache.spark.MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!
INFO Thread-1 org.apache.spark.storage.memory.MemoryStore - MemoryStore cleared
INFO Thread-1 org.apache.spark.storage.BlockManager - BlockManager stopped
INFO Thread-1 org.apache.spark.storage.BlockManagerMaster - BlockManagerMaster stopped
INFO dispatcher-event-loop-4 org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!
INFO Thread-1 org.apache.spark.SparkContext - Successfully stopped SparkContext
INFO Thread-1 org.apache.spark.util.ShutdownHookManager - Shutdown hook called
INFO Thread-1 org.apache.spark.util.ShutdownHookManager - Deleting directory C:\Users\wulong\AppData\Local\Temp\spark-b2ec7cc6-755d-4461-9b9b-87b26e126f3b
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 24309 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 38727 failed due to [test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 38729 failed due to [test,0]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,3]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 38730 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 45770 failed due to [test,0]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,3]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 45771 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 48316 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 63661 failed due to [test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 63663 failed due to [test,0]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,3]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 63664 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 69694 failed due to [test,0]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,3]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 69695 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 72137 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 91087 failed due to [test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 91090 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 94184 failed due to [test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 94185 failed due to [test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 94186 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 96487 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 122341 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 141394 failed due to [test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,5]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 141397 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 142044 failed due to [test,0]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,2]: org.apache.kafka.common.errors.NotLeaderForPartitionException,[test,3]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 142045 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 143119 failed due to [test,3]: org.apache.kafka.common.errors.NotLeaderForPartitionException
WARN main kafka.producer.async.DefaultEventHandler - Produce request with correlation id 143120 failed due to [test,0]: org.apache.kafka.common.errors.NotLeaderForPartitionException
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 143121 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 146147 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
WARN main kafka.producer.async.DefaultEventHandler - Failed to send producer request with correlation id 154076 to broker 2 with data for partitions [test,2]
java.io.IOException: 远程主机强迫关闭了一个现有的连接。
	at sun.nio.ch.SocketDispatcher.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:55)
	at sun.nio.ch.IOUtil.write(IOUtil.java:148)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:504)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:502)
	at org.apache.kafka.common.network.ByteBufferSend.writeTo(ByteBufferSend.java:57)
	at kafka.network.RequestOrResponseSend.writeCompletely(RequestOrResponseSend.scala:50)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:113)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:80)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:79)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:110)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:110)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:110)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:109)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:109)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:109)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:275)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:113)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:105)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:105)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:78)
	at kafka.producer.Producer.send(Producer.scala:78)
	at kafka.javaapi.producer.Producer.send(Producer.scala:44)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.sendListMassage(Flow_KafkaProdecer_Job.java:73)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.main(Flow_KafkaProdecer_Job.java:137)
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
WARN main kafka.producer.async.DefaultEventHandler - Failed to send producer request with correlation id 154077 to broker 1 with data for partitions [test,4],[test,1],[test,5]
java.io.IOException: 远程主机强迫关闭了一个现有的连接。
	at sun.nio.ch.SocketDispatcher.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:55)
	at sun.nio.ch.IOUtil.write(IOUtil.java:148)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:504)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:502)
	at org.apache.kafka.common.network.ByteBufferSend.writeTo(ByteBufferSend.java:57)
	at kafka.network.RequestOrResponseSend.writeCompletely(RequestOrResponseSend.scala:50)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:113)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:80)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:79)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:110)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:110)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:110)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:109)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:109)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:109)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:275)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:113)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:105)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:105)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:78)
	at kafka.producer.Producer.send(Producer.scala:78)
	at kafka.javaapi.producer.Producer.send(Producer.scala:44)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.sendListMassage(Flow_KafkaProdecer_Job.java:73)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.main(Flow_KafkaProdecer_Job.java:137)
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
WARN main kafka.producer.async.DefaultEventHandler - Failed to send producer request with correlation id 154078 to broker 0 with data for partitions [test,0],[test,3]
java.io.IOException: 远程主机强迫关闭了一个现有的连接。
	at sun.nio.ch.SocketDispatcher.writev0(Native Method)
	at sun.nio.ch.SocketDispatcher.writev(SocketDispatcher.java:55)
	at sun.nio.ch.IOUtil.write(IOUtil.java:148)
	at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:504)
	at java.nio.channels.SocketChannel.write(SocketChannel.java:502)
	at org.apache.kafka.common.network.ByteBufferSend.writeTo(ByteBufferSend.java:57)
	at kafka.network.RequestOrResponseSend.writeCompletely(RequestOrResponseSend.scala:50)
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:113)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:80)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:79)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SyncProducer.scala:110)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:110)
	at kafka.producer.SyncProducer$$anonfun$send$1$$anonfun$apply$mcV$sp$1.apply(SyncProducer.scala:110)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply$mcV$sp(SyncProducer.scala:109)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:109)
	at kafka.producer.SyncProducer$$anonfun$send$1.apply(SyncProducer.scala:109)
	at kafka.metrics.KafkaTimer.time(KafkaTimer.scala:33)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:108)
	at kafka.producer.async.DefaultEventHandler.kafka$producer$async$DefaultEventHandler$$send(DefaultEventHandler.scala:275)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:113)
	at kafka.producer.async.DefaultEventHandler$$anonfun$dispatchSerializedData$2.apply(DefaultEventHandler.scala:105)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:99)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
	at kafka.producer.async.DefaultEventHandler.dispatchSerializedData(DefaultEventHandler.scala:105)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:78)
	at kafka.producer.Producer.send(Producer.scala:78)
	at kafka.javaapi.producer.Producer.send(Producer.scala:44)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.sendListMassage(Flow_KafkaProdecer_Job.java:73)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.main(Flow_KafkaProdecer_Job.java:137)
INFO main kafka.producer.async.DefaultEventHandler - Back off for 100 ms before retrying send. Remaining retries = 2
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 154079 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
WARN main kafka.client.ClientUtils$ - Fetching topic metadata with correlation id 154079 for topics [Set(test)] from broker [BrokerEndPoint(1,192.168.145.102,9092)] failed
java.nio.channels.ClosedChannelException
	at kafka.network.BlockingChannel.send(BlockingChannel.scala:110)
	at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:80)
	at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:79)
	at kafka.producer.SyncProducer.send(SyncProducer.scala:124)
	at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:59)
	at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:83)
	at kafka.producer.async.DefaultEventHandler$$anonfun$handle$2.apply$mcV$sp(DefaultEventHandler.scala:84)
	at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:76)
	at kafka.utils.Logging$class.swallowError(Logging.scala:106)
	at kafka.utils.CoreUtils$.swallowError(CoreUtils.scala:47)
	at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:84)
	at kafka.producer.Producer.send(Producer.scala:78)
	at kafka.javaapi.producer.Producer.send(Producer.scala:44)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.sendListMassage(Flow_KafkaProdecer_Job.java:73)
	at com.hxqh.bigdata.kafka.Flow_KafkaProdecer_Job.main(Flow_KafkaProdecer_Job.java:137)
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 154079 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 167882 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 191779 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 216191 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 239882 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 263470 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 288014 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 310778 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 336426 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(2,192.168.145.103,9092) with correlation id 362486 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.103:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.103:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 387722 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(0,192.168.145.101,9092) with correlation id 412134 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.101:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.101:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
INFO main kafka.client.ClientUtils$ - Fetching metadata from broker BrokerEndPoint(1,192.168.145.102,9092) with correlation id 436476 for 1 topic(s) Set(test)
INFO main kafka.producer.SyncProducer - Connected to 192.168.145.102:9092 for producing
INFO main kafka.producer.SyncProducer - Disconnecting from 192.168.145.102:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node2:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node1:9092
INFO main kafka.producer.SyncProducer - Disconnecting from node3:9092
INFO main kafka.producer.SyncProducer - Connected to node3:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node2:9092 for producing
INFO main kafka.producer.SyncProducer - Connected to node1:9092 for producing
